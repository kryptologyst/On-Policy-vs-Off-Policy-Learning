# On-Policy vs Off-Policy Learning

A comprehensive implementation comparing SARSA (on-policy) and Q-Learning (off-policy) algorithms with state-of-the-art features and visualization capabilities.

## ğŸš€ Features

- **Modern RL Framework**: Built with Gymnasium (successor to OpenAI Gym)
- **Algorithm Comparison**: Side-by-side comparison of SARSA and Q-Learning
- **Multiple Environments**: Support for FrozenLake, CartPole, MountainCar, and custom environments
- **Rich Visualizations**: Learning curves, value functions, policies, and epsilon decay plots
- **Interactive UI**: Streamlit web interface for easy experimentation
- **Configuration Management**: YAML-based configuration system
- **Type Safety**: Full type hints and modern Python practices
- **Extensible Design**: Easy to add new algorithms and environments

## ğŸ“‹ Requirements

- Python 3.10+
- See `requirements.txt` for full dependency list

## ğŸ› ï¸ Installation

1. Clone the repository:
```bash
git clone https://github.com/kryptologyst/On-Policy-vs-Off-Policy-Learning.git
cd On-Policy-vs-Off-Policy-Learning
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ¯ Quick Start

### Command Line Interface

Train agents with default settings:
```bash
python src/main.py
```

Customize training parameters:
```bash
python src/main.py --episodes 2000 --alpha 0.05 --gamma 0.95 --env FrozenLake-v1
```

### Streamlit Web Interface

Launch the interactive web interface:
```bash
streamlit run src/streamlit_app.py
```

Then open your browser to `http://localhost:8501`

### Jupyter Notebook

For interactive development and analysis:
```bash
jupyter notebook notebooks/
```

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/           # RL agent implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â”œâ”€â”€ sarsa_agent.py
â”‚   â”‚   â””â”€â”€ qlearning_agent.py
â”‚   â”œâ”€â”€ envs/             # Environment definitions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ custom_envs.py
â”‚   â”œâ”€â”€ config_manager.py # Configuration management
â”‚   â”œâ”€â”€ visualization.py  # Plotting utilities
â”‚   â”œâ”€â”€ main.py          # CLI training script
â”‚   â””â”€â”€ streamlit_app.py # Web interface
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml      # Default configuration
â”œâ”€â”€ notebooks/           # Jupyter notebooks
â”œâ”€â”€ tests/               # Unit tests
â”œâ”€â”€ logs/                # Training logs
â”œâ”€â”€ checkpoints/         # Saved models
â”œâ”€â”€ plots/               # Generated plots
â”œâ”€â”€ requirements.txt     # Dependencies
â””â”€â”€ README.md           # This file
```

## ğŸ§  Algorithms

### SARSA (State-Action-Reward-State-Action)
- **Type**: On-policy temporal difference learning
- **Update Rule**: Q(s,a) = Q(s,a) + Î±[r + Î³Q(s',a') - Q(s,a)]
- **Characteristics**: Learns from actions actually taken, more conservative

### Q-Learning
- **Type**: Off-policy temporal difference learning  
- **Update Rule**: Q(s,a) = Q(s,a) + Î±[r + Î³max_a'Q(s',a') - Q(s,a)]
- **Characteristics**: Learns from optimal actions, more optimistic

## ğŸŒ Supported Environments

- **FrozenLake-v1**: Classic grid world navigation task
- **CartPole-v1**: Balance a pole on a cart
- **MountainCar-v0**: Drive a car up a mountain
- **Acrobot-v1**: Swing up a two-link pendulum
- **GridWorld**: Custom 5x5 grid world
- **CliffWalking**: Cliff walking environment from Sutton & Barto

## ğŸ“Š Visualizations

The framework provides comprehensive visualizations:

- **Learning Curves**: Episode rewards over time with moving averages
- **Value Functions**: Heatmaps showing state values
- **Policies**: Action maps showing optimal actions per state
- **Epsilon Decay**: Exploration rate decay over episodes
- **Performance Comparison**: Side-by-side algorithm comparison

## âš™ï¸ Configuration

Configuration is managed through YAML files. Key parameters:

```yaml
training:
  episodes: 1000
  alpha: 0.1      # Learning rate
  gamma: 0.99     # Discount factor
  epsilon: 0.1    # Initial exploration rate
  epsilon_decay: 0.995
  epsilon_min: 0.01

environment:
  name: "FrozenLake-v1"
  kwargs:
    is_slippery: false

visualization:
  plot_learning_curves: true
  plot_value_functions: true
  plot_policies: true
  save_plots: true
```

## ğŸ”¬ Usage Examples

### Basic Training
```python
from src.agents import SARSAAgent, QLearningAgent
from src.envs import make_env

# Create environment
env = make_env("FrozenLake-v1")

# Initialize agents
sarsa_agent = SARSAAgent(env)
qlearning_agent = QLearningAgent(env)

# Train agents
sarsa_metrics = sarsa_agent.train(1000)
qlearning_metrics = qlearning_agent.train(1000)

# Compare results
print(f"SARSA final reward: {np.mean(sarsa_metrics.episode_rewards[-100:]):.2f}")
print(f"Q-Learning final reward: {np.mean(qlearning_metrics.episode_rewards[-100:]):.2f}")
```

### Custom Environment
```python
from src.envs import GridWorldEnv

# Create custom environment
env = GridWorldEnv(size=5, goal_reward=10.0)

# Train agent
agent = SARSAAgent(env)
metrics = agent.train(500)
```

### Visualization
```python
from src.visualization import RLVisualizer

# Create visualizer
viz = RLVisualizer()

# Plot learning curves
viz.plot_learning_curves({
    'SARSA': sarsa_metrics.episode_rewards,
    'Q-Learning': qlearning_metrics.episode_rewards
})
```

## ğŸ§ª Testing

Run unit tests:
```bash
pytest tests/
```

Run with coverage:
```bash
pytest tests/ --cov=src
```

## ğŸ“ˆ Performance Tips

1. **Hyperparameter Tuning**: Experiment with learning rates (0.01-0.3) and discount factors (0.9-0.99)
2. **Exploration Strategy**: Adjust epsilon decay for different exploration schedules
3. **Environment Selection**: Start with deterministic environments (is_slippery=False) for easier learning
4. **Episode Count**: Use at least 1000 episodes for stable learning curves

## ğŸ”§ Advanced Features

### Adding New Algorithms
1. Inherit from `BaseAgent`
2. Implement `update_q_value()` and `train_episode()` methods
3. Add to agent registry in `src/agents/__init__.py`

### Custom Environments
1. Inherit from `gymnasium.Env`
2. Implement required methods: `reset()`, `step()`, `render()`
3. Add to environment factory in `src/envs/__init__.py`

### Logging and Monitoring
- TensorBoard integration for training metrics
- Weights & Biases support for experiment tracking
- Automatic checkpoint saving

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“š References

- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press.
- Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine learning, 8(3-4), 279-292.
- Rummery, G. A., & Niranjan, M. (1994). On-line Q-learning using connectionist systems.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- OpenAI Gym/Gymnasium team for the RL environment framework
- The reinforcement learning community for algorithms and insights
- Contributors and users who provide feedback and improvements
# On-Policy-vs-Off-Policy-Learning
